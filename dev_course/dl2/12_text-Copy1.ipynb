{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_11a import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the IMDB dataset that consists of 50,000 labeled reviews of movies (positive or negative) and 50,000 unlabelled ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = datasets.untar_data(datasets.URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/home/jupyter/.fastai/data/imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/jupyter/.fastai/data/imdb/test'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/tmp_clas'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/imdb.vocab'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/README'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/unsup'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/train'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/tmp_lm')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a subclass of `ItemList` that will read the texts in the corresponding filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_file(fn): \n",
    "    with open(fn, 'r', encoding = 'utf8') as f: return f.read()\n",
    "    \n",
    "class TextList(ItemList):\n",
    "    @classmethod\n",
    "    def from_files(cls, path, extensions='.txt', recurse=True, include=None, **kwargs):\n",
    "        return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)\n",
    "    \n",
    "    def get(self, i):\n",
    "        if isinstance(i, Path): return read_file(i)\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ItemList??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_files??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case there are some text log files, we restrict the ones we take to the training, test, and unsupervised folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should expect a total of 100,000 texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(il.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first one as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I saw this on television more years ago than I can remember, but never forgot the performance of Sammy Davis, Jr. I just by chance thought to look for it on video. This rendition of Porgy and Bess is a treasure. I would love to see it again and introduce my son to it as well. I just can't imagine why it is not heralded as one of the greatest performances Sammy Davis, Jr. every gave. Whoever is responsible for not bringing this to audiences should be ashamed of his/her ignorance. I will continue to look for it though. Maybe the execs responsible for such things will come to realize the forgotten work of so many African American actors.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = il[1]\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text classification, we will split by the grand parent folder as before, but for language modeling, we take all the texts and just put 10% aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplitData\n",
       "Train: TextList (90111 items)\n",
       "[PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/6778_8.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/354_10.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/10746_8.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/4801_10.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/6579_7.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/4187_7.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/3827_9.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/12157_7.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/9054_8.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/8103_10.txt')...]\n",
       "Path: /home/jupyter/.fastai/data/imdb\n",
       "Valid: TextList (9889 items)\n",
       "[PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/7690_10.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/9764_10.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/6263_10.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/10610_7.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/4269_7.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/8972_7.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/8109_9.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/3172_10.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/1660_8.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/test/pos/2483_9.txt')...]\n",
       "Path: /home/jupyter/.fastai/data/imdb"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tokenize the dataset first, which is splitting a sentence in individual tokens. Those tokens are the basic words or punctuation signs with a few tweaks: don't for instance is split between do and n't. We will use a processor for this, in conjunction with the [spacy library](https://spacy.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import spacy,html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before even tokenizeing, we will apply a bit of preprocessing on the texts to clean them up (we saw the one up there had some HTML code). These rules are applied before we split the sentences in tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#special tokens\n",
    "UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
    "\n",
    "def sub_br(t):\n",
    "    \"Replaces the <br /> by \\n\"\n",
    "    re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\n",
    "    return re_br.sub(\"\\n\", t)\n",
    "\n",
    "def spec_add_spaces(t):\n",
    "    \"Add spaces around / and #\"\n",
    "    return re.sub(r'([/#])', r' \\1 ', t)\n",
    "\n",
    "def rm_useless_spaces(t):\n",
    "    \"Remove multiple spaces\"\n",
    "    return re.sub(' {2,}', ' ', t)\n",
    "\n",
    "def replace_rep(t):\n",
    "    \"Replace repetitions at the character level: cccc -> TK_REP 4 c\"\n",
    "    def _replace_rep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_REP} {len(cc)+1} {c} '\n",
    "    re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
    "    return re_rep.sub(_replace_rep, t)\n",
    "    \n",
    "def replace_wrep(t):\n",
    "    \"Replace word repetitions: word word word -> TK_WREP 3 word\"\n",
    "    def _replace_wrep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_WREP} {len(cc.split())+1} {c} '\n",
    "    re_wrep = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
    "    return re_wrep.sub(_replace_wrep, t)\n",
    "\n",
    "def fixup_text(x):\n",
    "    \"Various messy things we've seen in documents\"\n",
    "    re1 = re.compile(r'  +')\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "    \n",
    "default_pre_rules = [fixup_text, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces, sub_br]\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' xxrep 4 c '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_rep('cccc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' xxwrep 5 word  '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_wrep('word word word word word ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rules are applied after the tokenization on the list of tokens.  (they apply at the word level so apply after you have split into words/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_all_caps(x):\n",
    "    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t.isupper() and len(t) > 1: res.append(TK_UP); res.append(t.lower())\n",
    "        else: res.append(t)\n",
    "    return res\n",
    "\n",
    "def deal_caps(x):\n",
    "    \"Replace all Capitalized tokens in by their lower version and add `TK_MAJ` before.\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t == '': continue\n",
    "        if t[0].isupper() and len(t) > 1 and t[1:].islower(): res.append(TK_MAJ)\n",
    "        res.append(t.lower())\n",
    "    return res\n",
    "\n",
    "\n",
    "# these are really important tokens because tell RNN to reset state. we are not longer talking about the same topic after EOS\n",
    "def add_eos_bos(x): return [BOS] + x + [EOS]\n",
    "\n",
    "default_post_rules = [deal_caps, replace_all_caps, add_eos_bos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'xxup', 'am', 'xxup', 'shouting']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_all_caps(['I', 'AM', 'SHOUTING'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxmaj', 'my', 'name', 'is', 'xxmaj', 'jeremy']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deal_caps(['My', 'name', 'is', 'Jeremy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deal_caps([\"I\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since tokenizing and applying those rules takes a bit of time, we'll parallelize it using `ProcessPoolExecutor` to go faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from spacy.symbols import ORTH\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def parallel(func, arr, max_workers=4):\n",
    "    if max_workers<2: results = list(progress_bar(map(func, enumerate(arr)), total=len(arr)))\n",
    "    else:\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "            return list(progress_bar(ex.map(func, enumerate(arr)), total=len(arr)))\n",
    "    if any([o is not None for o in results]): return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenizeProcessor(Processor):\n",
    "    def __init__(self, lang=\"en\", chunksize=2000, pre_rules=None, post_rules=None, max_workers=4): \n",
    "        self.chunksize,self.max_workers = chunksize,max_workers\n",
    "        self.tokenizer = spacy.blank(lang).tokenizer\n",
    "        for w in default_spec_tok:\n",
    "            self.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "        self.pre_rules  = default_pre_rules  if pre_rules  is None else pre_rules\n",
    "        self.post_rules = default_post_rules if post_rules is None else post_rules\n",
    "\n",
    "    def proc_chunk(self, args):\n",
    "        i,chunk = args\n",
    "        chunk = [compose(t, self.pre_rules) for t in chunk]\n",
    "        docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)]\n",
    "        docs = [compose(t, self.post_rules) for t in docs]\n",
    "        return docs\n",
    "\n",
    "    def __call__(self, items): \n",
    "        toks = []\n",
    "        if isinstance(items[0], Path): items = [read_file(i) for i in items]\n",
    "        chunks = [items[i: i+self.chunksize] for i in (range(0, len(items), self.chunksize))]\n",
    "        toks = parallel(self.proc_chunk, chunks, max_workers=self.max_workers)\n",
    "        return sum(toks, [])\n",
    "    \n",
    "    def proc1(self, item): return self.proc_chunk([item])[0]\n",
    "    \n",
    "    def deprocess(self, toks): return [self.deproc1(tok) for tok in toks]\n",
    "    def deproc1(self, tok):    return \" \".join(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TokenizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I saw this on television more years ago than I can remember, but never forgot the performance of Sammy Davis, Jr. I just by chance thought to look for it on video. This rendition of Porgy and Bess is a treasure. I would love to see it again and intro'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'xxbos • i • saw • this • on • television • more • years • ago • than • i • can • remember • , • but • never • forgot • the • performance • of • xxmaj • sammy • xxmaj • davis • , • xxmaj • jr. • i • just • by • chance • thought • to • look • for • it • on • video • . • xxmaj • this • rendition • of • xxmaj • porgy • and • xxmaj • bess • is • a • treasure • . • i • would • love • to • see • it • aga'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' • '.join(tp(il[:100])[1])[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s a w'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp.deproc1('saw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s a w', 't h i s']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp.deprocess([\"saw\", \"this\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that we are not removing stop words or stemmig (removig \"ing\"s) which is quite common in traditional NLP. It's a terrible idea because you are losing valuable info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have tokenized our texts, we replace each token by an individual number, this is called numericalizing. Again, we do this with a processor (not so different from the `CategoryProcessor`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import collections\n",
    "\n",
    "class NumericalizeProcessor(Processor):\n",
    "    def __init__(self, vocab=None, max_vocab=60000, min_freq=2): \n",
    "        self.vocab,self.max_vocab,self.min_freq = vocab,max_vocab,min_freq\n",
    "    \n",
    "    def __call__(self, items):\n",
    "        #The vocab is defined on the first use.\n",
    "        if self.vocab is None:\n",
    "            freq = Counter(p for o in items for p in o)\n",
    "            self.vocab = [o for o,c in freq.most_common(self.max_vocab) if c >= self.min_freq]\n",
    "            for o in reversed(default_spec_tok):\n",
    "                if o in self.vocab: self.vocab.remove(o)\n",
    "                self.vocab.insert(0, o)\n",
    "        if getattr(self, 'otoi', None) is None:\n",
    "            self.otoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.vocab)}) \n",
    "        return [self.proc1(o) for o in items]\n",
    "    def proc1(self, item):  return [self.otoi[o] for o in item]\n",
    "    \n",
    "    def deprocess(self, idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deproc1(idx) for idx in idxs]\n",
    "    def deproc1(self, idx): return [self.vocab[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do language modeling, we will infer the labels from the text during training, so there's no need to label. The training loop expects labels however, so we need to add dummy ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_tok,proc_num = TokenizeProcessor(max_workers=8),NumericalizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_num.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='46' class='' max='46', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [46/46 00:47<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:06<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 s, sys: 2.98 s, total: 23.5 s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%time ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the items have been processed they will become list of numbers, we can still access the underlying raw data in `x_obj` (or `y_obj` for the targets, but we don't have any here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbos * this comment will probably have spoilers ! ! i checked the spoilers box just in case but it might not have spoilers , but be aware anyways if i say something that you might consider a spoiler and i don't ! * xxmaj wow ... best game since xxmaj super xxmaj mario 64 . i got this game the first day it came out , and before i got it , i went on some gaming websites to look at its ratings ( yes , they already reviewed it before it came out ) , and i was shocked . i was expecting something like xxmaj sunshine because lately all the xxmaj mario games have kind of been getting worse and worse . xxmaj but this one totally beat the other games . xxmaj the scores on this game even beat xxmaj halo 3 ! xxmaj it 's simply amazing . \\n\\n story : xxmaj not the best , xxmaj mario games are never known for their plots , and this one is n't really much of a difference . xxmaj bowser once again kidnaps xxmaj peach but this time invades the xxmaj mushroom xxmaj kingdom on a festival celebrated once every century and gets a flying saucer and it shoots lasers on the ground and then they put anchors inside the ground and rip off the castle and its foundation into outer space . \\n\\n graphics : xxmaj absolutely gorgeous , the best graphics on xxmaj wii so far . xxmaj the water effects are really nice too , if you ever see the water , it looks so real because the effects they put in it . \\n\\n music : xxmaj simply amazing , the music in this game is orchestrated . xxmaj not all of it , but even the ones that are n't orchestrated still sound very nice . \\n\\n gameplay : xxmaj very entertaining , keeps you wanting to play more and more , and for me , since usually i tend to be very frustrated with games if i die a lot , i mean , i 'm definitely not the only one , but this game for me did n't have that a lot . xxmaj now it still kind of did , but not to the point like xxmaj mario 64 where i totally go crazy and end up turning off the game because it made me mad , this one never did that . \\n\\n difficulty : xxmaj there are two types of difficulty , because you can just technically beat the game with just 60 stars , which i think is kind of easy , but to beat the game 100 % , you have to get 120 , like always , and that quest is way harder , but at the same time still very fun . \\n\\n length : xxmaj good length , definitely not too short . xxmaj it took me around 15 - 16 hours to beat it with just 60 stars , but 120 took me more like 45 - 50 hours . xxmaj now i did n't play it constantly that much , i played first a lot one day , then the next couple of days just a couple hours each day , then the next two days i played it all day . xxmaj to me though , i think the quest for 60 stars was a tiny bit short , so if you want a long game , i suggest getting all of them . \\n\\n presentation : xxmaj now , the cinematic scenes in this movie are superb . xxmaj the way they are , it just looks so much like something i would see in the theaters . xxmaj they are hands down the best cinematic scenes on any xxmaj mario game so far . \\n\\n overall : xxmaj this game is definitely a great game , and i advise you to not wait until xxmaj christmas , get it as soon as possible . xxmaj this game also is a good mixture of old and new , you 'll have some xxunk levels where you move just side to side , or up and down because they 'll change on gravity , but even the music has some old classics in it . xxmaj so this game seems to be like a masterpiece . xxmaj to me , it 's my favorite xxmaj mario game of all time , but others may disagree . xxmaj but it 's still a great game . i recommend every gamer who wants to have fun in a game to get this . xxeos\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.train.x_obj(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the preprocessing tajes time, we save the intermediate result using pickle. Don't use any lambda functions in your processors or they won't be able to pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path/'ld.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ld.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2,\n",
       "  195,\n",
       "  19,\n",
       "  947,\n",
       "  105,\n",
       "  261,\n",
       "  41,\n",
       "  1068,\n",
       "  50,\n",
       "  50,\n",
       "  18,\n",
       "  4789,\n",
       "  8,\n",
       "  1068,\n",
       "  883,\n",
       "  56,\n",
       "  17,\n",
       "  438,\n",
       "  30,\n",
       "  16,\n",
       "  251,\n",
       "  37,\n",
       "  41,\n",
       "  1068,\n",
       "  10,\n",
       "  30,\n",
       "  43,\n",
       "  1855,\n",
       "  3806,\n",
       "  62,\n",
       "  18,\n",
       "  157,\n",
       "  158,\n",
       "  20,\n",
       "  32,\n",
       "  251,\n",
       "  1251,\n",
       "  12,\n",
       "  1324,\n",
       "  11,\n",
       "  18,\n",
       "  3310,\n",
       "  50,\n",
       "  195,\n",
       "  7,\n",
       "  1373,\n",
       "  92,\n",
       "  138,\n",
       "  473,\n",
       "  252,\n",
       "  7,\n",
       "  1266,\n",
       "  7,\n",
       "  4315,\n",
       "  11784,\n",
       "  9,\n",
       "  18,\n",
       "  209,\n",
       "  19,\n",
       "  473,\n",
       "  8,\n",
       "  107,\n",
       "  271,\n",
       "  16,\n",
       "  410,\n",
       "  61,\n",
       "  10,\n",
       "  11,\n",
       "  181,\n",
       "  18,\n",
       "  209,\n",
       "  16,\n",
       "  10,\n",
       "  18,\n",
       "  434,\n",
       "  34,\n",
       "  65,\n",
       "  11346,\n",
       "  15935,\n",
       "  14,\n",
       "  185,\n",
       "  45,\n",
       "  114,\n",
       "  2974,\n",
       "  36,\n",
       "  445,\n",
       "  10,\n",
       "  46,\n",
       "  494,\n",
       "  6761,\n",
       "  16,\n",
       "  181,\n",
       "  16,\n",
       "  410,\n",
       "  61,\n",
       "  33,\n",
       "  10,\n",
       "  11,\n",
       "  18,\n",
       "  25,\n",
       "  2230,\n",
       "  9,\n",
       "  18,\n",
       "  25,\n",
       "  1037,\n",
       "  158,\n",
       "  53,\n",
       "  7,\n",
       "  5048,\n",
       "  106,\n",
       "  4618,\n",
       "  44,\n",
       "  8,\n",
       "  7,\n",
       "  4315,\n",
       "  1489,\n",
       "  41,\n",
       "  265,\n",
       "  13,\n",
       "  98,\n",
       "  401,\n",
       "  460,\n",
       "  11,\n",
       "  460,\n",
       "  9,\n",
       "  7,\n",
       "  30,\n",
       "  19,\n",
       "  42,\n",
       "  483,\n",
       "  1444,\n",
       "  8,\n",
       "  101,\n",
       "  1489,\n",
       "  9,\n",
       "  7,\n",
       "  8,\n",
       "  4161,\n",
       "  34,\n",
       "  19,\n",
       "  473,\n",
       "  76,\n",
       "  1444,\n",
       "  7,\n",
       "  18073,\n",
       "  389,\n",
       "  50,\n",
       "  7,\n",
       "  16,\n",
       "  22,\n",
       "  346,\n",
       "  525,\n",
       "  9,\n",
       "  24,\n",
       "  81,\n",
       "  96,\n",
       "  7,\n",
       "  37,\n",
       "  8,\n",
       "  138,\n",
       "  10,\n",
       "  7,\n",
       "  4315,\n",
       "  1489,\n",
       "  38,\n",
       "  133,\n",
       "  560,\n",
       "  28,\n",
       "  80,\n",
       "  1666,\n",
       "  10,\n",
       "  11,\n",
       "  19,\n",
       "  42,\n",
       "  15,\n",
       "  35,\n",
       "  84,\n",
       "  94,\n",
       "  13,\n",
       "  12,\n",
       "  1555,\n",
       "  9,\n",
       "  7,\n",
       "  28384,\n",
       "  302,\n",
       "  193,\n",
       "  8442,\n",
       "  7,\n",
       "  17111,\n",
       "  30,\n",
       "  19,\n",
       "  75,\n",
       "  20588,\n",
       "  8,\n",
       "  7,\n",
       "  19354,\n",
       "  7,\n",
       "  4803,\n",
       "  34,\n",
       "  12,\n",
       "  1341,\n",
       "  7180,\n",
       "  302,\n",
       "  190,\n",
       "  1108,\n",
       "  11,\n",
       "  241,\n",
       "  12,\n",
       "  1664,\n",
       "  18302,\n",
       "  11,\n",
       "  16,\n",
       "  3085,\n",
       "  19070,\n",
       "  34,\n",
       "  8,\n",
       "  1526,\n",
       "  11,\n",
       "  115,\n",
       "  46,\n",
       "  298,\n",
       "  14242,\n",
       "  973,\n",
       "  8,\n",
       "  1526,\n",
       "  11,\n",
       "  1565,\n",
       "  141,\n",
       "  8,\n",
       "  2106,\n",
       "  11,\n",
       "  114,\n",
       "  6879,\n",
       "  104,\n",
       "  3149,\n",
       "  767,\n",
       "  9,\n",
       "  24,\n",
       "  2929,\n",
       "  96,\n",
       "  7,\n",
       "  448,\n",
       "  1548,\n",
       "  10,\n",
       "  8,\n",
       "  138,\n",
       "  2929,\n",
       "  34,\n",
       "  7,\n",
       "  31047,\n",
       "  51,\n",
       "  247,\n",
       "  9,\n",
       "  7,\n",
       "  8,\n",
       "  998,\n",
       "  307,\n",
       "  38,\n",
       "  84,\n",
       "  359,\n",
       "  117,\n",
       "  10,\n",
       "  62,\n",
       "  32,\n",
       "  144,\n",
       "  83,\n",
       "  8,\n",
       "  998,\n",
       "  10,\n",
       "  16,\n",
       "  308,\n",
       "  51,\n",
       "  164,\n",
       "  106,\n",
       "  8,\n",
       "  307,\n",
       "  46,\n",
       "  298,\n",
       "  17,\n",
       "  16,\n",
       "  9,\n",
       "  24,\n",
       "  237,\n",
       "  96,\n",
       "  7,\n",
       "  346,\n",
       "  525,\n",
       "  10,\n",
       "  8,\n",
       "  237,\n",
       "  17,\n",
       "  19,\n",
       "  473,\n",
       "  15,\n",
       "  13325,\n",
       "  9,\n",
       "  7,\n",
       "  37,\n",
       "  44,\n",
       "  13,\n",
       "  16,\n",
       "  10,\n",
       "  30,\n",
       "  76,\n",
       "  8,\n",
       "  707,\n",
       "  20,\n",
       "  38,\n",
       "  35,\n",
       "  13325,\n",
       "  152,\n",
       "  495,\n",
       "  70,\n",
       "  359,\n",
       "  9,\n",
       "  24,\n",
       "  11207,\n",
       "  96,\n",
       "  7,\n",
       "  70,\n",
       "  433,\n",
       "  10,\n",
       "  942,\n",
       "  32,\n",
       "  1680,\n",
       "  14,\n",
       "  316,\n",
       "  68,\n",
       "  11,\n",
       "  68,\n",
       "  10,\n",
       "  11,\n",
       "  28,\n",
       "  89,\n",
       "  10,\n",
       "  252,\n",
       "  633,\n",
       "  18,\n",
       "  2466,\n",
       "  14,\n",
       "  43,\n",
       "  70,\n",
       "  3726,\n",
       "  27,\n",
       "  1489,\n",
       "  62,\n",
       "  18,\n",
       "  716,\n",
       "  12,\n",
       "  189,\n",
       "  10,\n",
       "  18,\n",
       "  404,\n",
       "  10,\n",
       "  18,\n",
       "  167,\n",
       "  428,\n",
       "  37,\n",
       "  8,\n",
       "  82,\n",
       "  42,\n",
       "  10,\n",
       "  30,\n",
       "  19,\n",
       "  473,\n",
       "  28,\n",
       "  89,\n",
       "  88,\n",
       "  35,\n",
       "  41,\n",
       "  20,\n",
       "  12,\n",
       "  189,\n",
       "  9,\n",
       "  7,\n",
       "  166,\n",
       "  16,\n",
       "  152,\n",
       "  265,\n",
       "  13,\n",
       "  88,\n",
       "  10,\n",
       "  30,\n",
       "  37,\n",
       "  14,\n",
       "  8,\n",
       "  243,\n",
       "  53,\n",
       "  7,\n",
       "  4315,\n",
       "  11784,\n",
       "  135,\n",
       "  18,\n",
       "  483,\n",
       "  159,\n",
       "  946,\n",
       "  11,\n",
       "  148,\n",
       "  72,\n",
       "  1579,\n",
       "  141,\n",
       "  8,\n",
       "  473,\n",
       "  106,\n",
       "  16,\n",
       "  112,\n",
       "  89,\n",
       "  1172,\n",
       "  10,\n",
       "  19,\n",
       "  42,\n",
       "  133,\n",
       "  88,\n",
       "  20,\n",
       "  9,\n",
       "  24,\n",
       "  5855,\n",
       "  96,\n",
       "  7,\n",
       "  54,\n",
       "  38,\n",
       "  126,\n",
       "  2146,\n",
       "  13,\n",
       "  5855,\n",
       "  10,\n",
       "  106,\n",
       "  32,\n",
       "  78,\n",
       "  56,\n",
       "  2840,\n",
       "  1444,\n",
       "  8,\n",
       "  473,\n",
       "  27,\n",
       "  56,\n",
       "  2016,\n",
       "  423,\n",
       "  10,\n",
       "  79,\n",
       "  18,\n",
       "  122,\n",
       "  15,\n",
       "  265,\n",
       "  13,\n",
       "  754,\n",
       "  10,\n",
       "  30,\n",
       "  14,\n",
       "  1444,\n",
       "  8,\n",
       "  473,\n",
       "  1326,\n",
       "  1374,\n",
       "  10,\n",
       "  32,\n",
       "  41,\n",
       "  14,\n",
       "  99,\n",
       "  14363,\n",
       "  10,\n",
       "  53,\n",
       "  232,\n",
       "  10,\n",
       "  11,\n",
       "  20,\n",
       "  2887,\n",
       "  15,\n",
       "  116,\n",
       "  3795,\n",
       "  10,\n",
       "  30,\n",
       "  45,\n",
       "  8,\n",
       "  187,\n",
       "  75,\n",
       "  152,\n",
       "  70,\n",
       "  266,\n",
       "  9,\n",
       "  24,\n",
       "  1611,\n",
       "  96,\n",
       "  7,\n",
       "  67,\n",
       "  1611,\n",
       "  10,\n",
       "  428,\n",
       "  37,\n",
       "  117,\n",
       "  362,\n",
       "  9,\n",
       "  7,\n",
       "  16,\n",
       "  576,\n",
       "  89,\n",
       "  207,\n",
       "  1173,\n",
       "  23,\n",
       "  2941,\n",
       "  621,\n",
       "  14,\n",
       "  1444,\n",
       "  16,\n",
       "  27,\n",
       "  56,\n",
       "  2016,\n",
       "  423,\n",
       "  10,\n",
       "  30,\n",
       "  14363,\n",
       "  576,\n",
       "  89,\n",
       "  68,\n",
       "  53,\n",
       "  3720,\n",
       "  23,\n",
       "  1347,\n",
       "  621,\n",
       "  9,\n",
       "  7,\n",
       "  166,\n",
       "  18,\n",
       "  88,\n",
       "  35,\n",
       "  316,\n",
       "  16,\n",
       "  1375,\n",
       "  20,\n",
       "  94,\n",
       "  10,\n",
       "  18,\n",
       "  276,\n",
       "  107,\n",
       "  12,\n",
       "  189,\n",
       "  42,\n",
       "  271,\n",
       "  10,\n",
       "  115,\n",
       "  8,\n",
       "  386,\n",
       "  380,\n",
       "  13,\n",
       "  497,\n",
       "  56,\n",
       "  12,\n",
       "  380,\n",
       "  621,\n",
       "  273,\n",
       "  271,\n",
       "  10,\n",
       "  115,\n",
       "  8,\n",
       "  386,\n",
       "  126,\n",
       "  497,\n",
       "  18,\n",
       "  276,\n",
       "  16,\n",
       "  44,\n",
       "  271,\n",
       "  9,\n",
       "  7,\n",
       "  14,\n",
       "  89,\n",
       "  172,\n",
       "  10,\n",
       "  18,\n",
       "  122,\n",
       "  8,\n",
       "  2887,\n",
       "  28,\n",
       "  2016,\n",
       "  423,\n",
       "  25,\n",
       "  12,\n",
       "  2303,\n",
       "  244,\n",
       "  362,\n",
       "  10,\n",
       "  51,\n",
       "  62,\n",
       "  32,\n",
       "  204,\n",
       "  12,\n",
       "  215,\n",
       "  473,\n",
       "  10,\n",
       "  18,\n",
       "  1492,\n",
       "  401,\n",
       "  44,\n",
       "  13,\n",
       "  111,\n",
       "  9,\n",
       "  24,\n",
       "  2968,\n",
       "  96,\n",
       "  7,\n",
       "  166,\n",
       "  10,\n",
       "  8,\n",
       "  1333,\n",
       "  153,\n",
       "  17,\n",
       "  19,\n",
       "  29,\n",
       "  38,\n",
       "  957,\n",
       "  9,\n",
       "  7,\n",
       "  8,\n",
       "  116,\n",
       "  46,\n",
       "  38,\n",
       "  10,\n",
       "  16,\n",
       "  56,\n",
       "  308,\n",
       "  51,\n",
       "  94,\n",
       "  53,\n",
       "  158,\n",
       "  18,\n",
       "  73,\n",
       "  83,\n",
       "  17,\n",
       "  8,\n",
       "  2203,\n",
       "  9,\n",
       "  7,\n",
       "  46,\n",
       "  38,\n",
       "  922,\n",
       "  202,\n",
       "  8,\n",
       "  138,\n",
       "  1333,\n",
       "  153,\n",
       "  34,\n",
       "  120,\n",
       "  7,\n",
       "  4315,\n",
       "  473,\n",
       "  51,\n",
       "  247,\n",
       "  9,\n",
       "  24,\n",
       "  464,\n",
       "  96,\n",
       "  7,\n",
       "  19,\n",
       "  473,\n",
       "  15,\n",
       "  428,\n",
       "  12,\n",
       "  103,\n",
       "  473,\n",
       "  10,\n",
       "  11,\n",
       "  18,\n",
       "  4721,\n",
       "  32,\n",
       "  14,\n",
       "  37,\n",
       "  884,\n",
       "  382,\n",
       "  7,\n",
       "  1393,\n",
       "  10,\n",
       "  99,\n",
       "  16,\n",
       "  26,\n",
       "  545,\n",
       "  26,\n",
       "  636,\n",
       "  9,\n",
       "  7,\n",
       "  19,\n",
       "  473,\n",
       "  102,\n",
       "  15,\n",
       "  12,\n",
       "  67,\n",
       "  4112,\n",
       "  13,\n",
       "  176,\n",
       "  11,\n",
       "  184,\n",
       "  10,\n",
       "  32,\n",
       "  260,\n",
       "  41,\n",
       "  65,\n",
       "  0,\n",
       "  2075,\n",
       "  135,\n",
       "  32,\n",
       "  843,\n",
       "  56,\n",
       "  507,\n",
       "  14,\n",
       "  507,\n",
       "  10,\n",
       "  55,\n",
       "  72,\n",
       "  11,\n",
       "  202,\n",
       "  106,\n",
       "  46,\n",
       "  260,\n",
       "  677,\n",
       "  34,\n",
       "  7151,\n",
       "  10,\n",
       "  30,\n",
       "  76,\n",
       "  8,\n",
       "  237,\n",
       "  60,\n",
       "  65,\n",
       "  176,\n",
       "  1962,\n",
       "  17,\n",
       "  16,\n",
       "  9,\n",
       "  7,\n",
       "  51,\n",
       "  19,\n",
       "  473,\n",
       "  208,\n",
       "  14,\n",
       "  43,\n",
       "  53,\n",
       "  12,\n",
       "  950,\n",
       "  9,\n",
       "  7,\n",
       "  14,\n",
       "  89,\n",
       "  10,\n",
       "  16,\n",
       "  22,\n",
       "  77,\n",
       "  538,\n",
       "  7,\n",
       "  4315,\n",
       "  473,\n",
       "  13,\n",
       "  44,\n",
       "  75,\n",
       "  10,\n",
       "  30,\n",
       "  415,\n",
       "  226,\n",
       "  3040,\n",
       "  9,\n",
       "  7,\n",
       "  30,\n",
       "  16,\n",
       "  22,\n",
       "  152,\n",
       "  12,\n",
       "  103,\n",
       "  473,\n",
       "  9,\n",
       "  18,\n",
       "  405,\n",
       "  190,\n",
       "  15757,\n",
       "  49,\n",
       "  509,\n",
       "  14,\n",
       "  41,\n",
       "  266,\n",
       "  17,\n",
       "  12,\n",
       "  473,\n",
       "  14,\n",
       "  99,\n",
       "  19,\n",
       "  9,\n",
       "  3],\n",
       " 0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a bit of work to convert our `LabelList` in a `DataBunch` as we don't just want batches of IMDB reviews. We want to stream through all the texts concatenated. We also have to prepare the targets that are the newt words in the text. All of this is done with the next object called `LM_PreLoader`. At the beginning of each epoch, it'll shuffle the articles (if `shuffle=True`) and create a big stream by concatenating all of them. We divide this big stream in `bs` smaller streams. That we will read in chunks of bptt length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just using those for illustration purposes, they're not used otherwise.\n",
    "from IPython.display import display,HTML\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say our stream is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = \"\"\"\n",
    "In this notebook, we will go back over the example of classifying movie reviews we studied in part 1 and dig deeper under the surface. \n",
    "First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the Processor used in the data block API.\n",
    "Then we will study how we build a language model and train it.\\n\n",
    "\"\"\"\n",
    "tokens = np.array(tp([stream])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if we split it in 6 batches it would give something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>in</td>\n",
       "      <td>this</td>\n",
       "      <td>notebook</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>go</td>\n",
       "      <td>back</td>\n",
       "      <td>over</td>\n",
       "      <td>the</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>classifying</td>\n",
       "      <td>movie</td>\n",
       "      <td>reviews</td>\n",
       "      <td>we</td>\n",
       "      <td>studied</td>\n",
       "      <td>in</td>\n",
       "      <td>part</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>dig</td>\n",
       "      <td>deeper</td>\n",
       "      <td>under</td>\n",
       "      <td>the</td>\n",
       "      <td>surface</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>first</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>look</td>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>processing</td>\n",
       "      <td>steps</td>\n",
       "      <td>necessary</td>\n",
       "      <td>to</td>\n",
       "      <td>convert</td>\n",
       "      <td>text</td>\n",
       "      <td>into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>numbers</td>\n",
       "      <td>and</td>\n",
       "      <td>how</td>\n",
       "      <td>to</td>\n",
       "      <td>customize</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>by</td>\n",
       "      <td>doing</td>\n",
       "      <td>this</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>another</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>processor</td>\n",
       "      <td>used</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>data</td>\n",
       "      <td>block</td>\n",
       "      <td>api</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>then</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>study</td>\n",
       "      <td>how</td>\n",
       "      <td>we</td>\n",
       "      <td>build</td>\n",
       "      <td>a</td>\n",
       "      <td>language</td>\n",
       "      <td>model</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs,seq_len = 6,15\n",
    "d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if we have a `bptt` of 5, we would go over those three batches.  (mean 6 batches??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are taking a batch size of 6 lines. we have bptt of 5, so we are taking 5 tokens in each line.  We need the 5 tokens of each line from each batch to match up with its corresponding next tokens so that the rnn state makes sense.  So we see how see how the 1st line and the 7th line fit together here (they will be the first part of every batch so the state will continue.  Same with the second line and the 8th line, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>in</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>classifying</td>\n",
       "      <td>movie</td>\n",
       "      <td>reviews</td>\n",
       "      <td>we</td>\n",
       "      <td>studied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>first</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>numbers</td>\n",
       "      <td>and</td>\n",
       "      <td>how</td>\n",
       "      <td>to</td>\n",
       "      <td>customize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>another</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>then</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>study</td>\n",
       "      <td>how</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>notebook</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>part</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>dig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>look</td>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>processing</td>\n",
       "      <td>steps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>by</td>\n",
       "      <td>doing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>processor</td>\n",
       "      <td>used</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>we</td>\n",
       "      <td>build</td>\n",
       "      <td>a</td>\n",
       "      <td>language</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>back</td>\n",
       "      <td>over</td>\n",
       "      <td>the</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>deeper</td>\n",
       "      <td>under</td>\n",
       "      <td>the</td>\n",
       "      <td>surface</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>necessary</td>\n",
       "      <td>to</td>\n",
       "      <td>convert</td>\n",
       "      <td>text</td>\n",
       "      <td>into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>block</td>\n",
       "      <td>api</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs,bptt = 6,5\n",
    "for k in range(3):\n",
    "    d_tokens = np.array([tokens[i*seq_len + k*bptt:i*seq_len + (k+1)*bptt] for i in range(bs)])\n",
    "    df = pd.DataFrame(d_tokens)\n",
    "    display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This LM_PreLoader class creates our data loader. Our y for every x is just the x shifted over 1 since we are trying to predict the next word.  So our getitem gives us our x and then the y is x indexed over 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LM_PreLoader():\n",
    "    def __init__(self, data, bs=64, bptt=70, shuffle=False):\n",
    "        self.data,self.bs,self.bptt,self.shuffle = data,bs,bptt,shuffle\n",
    "        total_len = sum([len(t) for t in data.x])\n",
    "        self.n_batch = total_len // bs\n",
    "        self.batchify()\n",
    "    \n",
    "    def __len__(self): return ((self.n_batch-1) // self.bptt) * self.bs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = self.batched_data[idx % self.bs]\n",
    "        seq_idx = (idx // self.bs) * self.bptt\n",
    "        return source[seq_idx:seq_idx+self.bptt],source[seq_idx+1:seq_idx+self.bptt+1]\n",
    "    \n",
    "    def batchify(self):\n",
    "        texts = self.data.x\n",
    "        if self.shuffle: texts = texts[torch.randperm(len(texts))]\n",
    "        stream = torch.cat([tensor(t) for t in texts])\n",
    "        self.batched_data = stream[:self.n_batch * self.bs].view(self.bs, self.n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(LM_PreLoader(ll.valid, shuffle=True), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it all works ok: `x1`, `y1`, `x2` and `y2` should all be of size `bs`  by `bptt`. The texts in each row of `x1` should continue in `x2`. `y1` and `y2` should have the same texts as their `x` counterpart, shifted of one position to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(dl)\n",
    "x1,y1 = next(iter_dl)\n",
    "x2,y2 = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 70]), torch.Size([64, 70]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.size(),y1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = proc_num.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj lipstick is another glossy movie failure . i am trying to think of one good thing that i could say about the movie , and i am having trouble coming up with something . i guess the red dress that xxmaj margaux xxmaj hemingway was wearing in the end of the movie was the best part . xxmaj the writing and the script was not the worst that'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in x1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxmaj lipstick is another glossy movie failure . i am trying to think of one good thing that i could say about the movie , and i am having trouble coming up with something . i guess the red dress that xxmaj margaux xxmaj hemingway was wearing in the end of the movie was the best part . xxmaj the writing and the script was not the worst that i'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in y1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i have ever encountered , but it could have been a lot better . xxmaj lipstick was very pleasing to the eye to view . xxmaj the sets were very glossy and nice to look at . xxmaj the cast was okay . i felt like xxmaj anne xxmaj bancroft 's character was the only feasible character in the entire movie . xxmaj it was sad to see xxmaj chris\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in x2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"have ever encountered , but it could have been a lot better . xxmaj lipstick was very pleasing to the eye to view . xxmaj the sets were very glossy and nice to look at . xxmaj the cast was okay . i felt like xxmaj anne xxmaj bancroft 's character was the only feasible character in the entire movie . xxmaj it was sad to see xxmaj chris xxmaj\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in y2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's prepare some convenience function to do this quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_lm_dls(train_ds, valid_ds, bs, bptt, **kwargs):\n",
    "    return (DataLoader(LM_PreLoader(train_ds, bs, bptt, shuffle=True), batch_size=bs, **kwargs),\n",
    "            DataLoader(LM_PreLoader(valid_ds, bs, bptt, shuffle=False), batch_size=2*bs, **kwargs))\n",
    "\n",
    "def lm_databunchify(sd, bs, bptt, **kwargs):\n",
    "    return DataBunch(*get_lm_dls(sd.train, sd.valid, bs, bptt, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt = 64,70\n",
    "data = lm_databunchify(ll, bs, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we will want to tackle classification, gathering the data will be a bit different: first we will label our texts with the folder they come from, and then we will need to apply padding to batch them together. To avoid mixing very long texts with very short ones, we will also use `Sampler` to sort (with a bit of randomness for the training set) our samples by length.\n",
    "\n",
    "First the data block API calls shold look familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_cat = CategoryProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "CategoryProcessor??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='13' class='' max='13', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [13/13 00:13<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='13' class='' max='13', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [13/13 00:15<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test'])\n",
    "sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='test'))\n",
    "ll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path/'ll_clas.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ll_clas.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the labels seem consistent with the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"xxbos xxmaj in a performance both volatile and graceful , xxmaj al xxmaj pacino re - teams with xxmaj sea of xxmaj love director , xxmaj harold xxmaj becker . \\n\\n xxmaj as xxmaj new xxmaj york xxmaj mayor xxmaj john xxmaj pappas in xxmaj city xxmaj hall . \\n\\n a savvy thriller that s the first film ever shot inside the lower xxmaj manhattan structure that 's ground zero for the xxmaj city 's government . \\n\\n xxmaj that the other nyc locations provide the vivid settings as an idealistic mayoral aide ( xxmaj john xxmaj cusack ) follows a trail of subversion and cover - up that may loop back to the man he serves and reveres . \\n\\n xxmaj bridget xxmaj fonda , xxmaj danny xxmaj aiello , xxmaj martin xxmaj landau , xxmaj tony xxmaj franciosa and xxmaj david xxmaj paymer add more starry brilliance to this gripping tale of power . \\n\\n xxmaj and the power behind power . xxeos\",\n",
       "  'pos'),\n",
       " ('xxbos a little girl lives with her father and brother in the middle of the countryside . xxmaj this little girl xxmaj rosalie has some psychotic tendencies as the movie opens with her feeding kittens to some kind of creatures in the cemetery , and she has recently lost her mother who went crazy but whilst alive enjoyed staying in the woods all night . xxmaj the premise of the film has a new young lady coming to xxmaj rosalie to take care of her . xxmaj she is introduced to the evil of the woods while driving and , imagine the suspense here , experiences a huge blue barrel falling over the side of a cliff to somehow stop her car dead in its tracks . xxmaj from there she walks to the nearest house and discovers xxmaj mrs. xxmaj whitfield who then goes into a whole lot of explanation about xxmaj rosalie and her family . xxmaj the earnestness exuded by the xxmaj mrs. xxmaj whitfield character has to be seen to be believed . xxmaj well , the young lady meets up with the child and we soon learn that not only is she strange but everyone in the film is very bizarre as well . xxmaj they all do share one thing in common which is none of them ever heard of an acting school . xxmaj none of these people can act - as evidenced by the few vehicles any of them in the entire film appeared in before or since - and all of them look like they have little idea what is going on , pause to remember lines , and have all the conviction of a paper bag . xxmaj the director plods through the material in a slow pace with this horrible piano music xxunk here and there at things that are suppose to be scary . xxmaj it takes us a bit before we get to a couple of murders by the creature friends , but by that time i did n\\'t care . xxmaj the murders are not convincing either , and truth be told the whole film looks like someone through it together on their friend \\'s farm with the people and things on hand there . xxmaj that all being said the ending does have some creepy aspects to it though we do n\\'t learn one darn thing about why xxmaj rosalie is like this or more importantly who the creature with the cheap masks are . xxmaj cheap does n\\'t even begin to describe the budget here with . xxmaj it basically is a couple old xxunk and some sheds at the end and of course the woods . xxmaj someone lent the director a couple old cars too . xxmaj no special effects of any kind and only the most minimal make - up . xxmaj there are so many guffaws / ridiculous moments to list , but i will just list a few here that at the very least made me chuckle from the lack of aptitude from the creative powers involved : xxunk the gardener \\'s body well after he has been \" slain \" . xxmaj len comes in and sees him butchered and you can see his fat belly heave with life . 2)the dying scene at the end where the actress playing xxmaj rosalie is killed . xxmaj she looks like she is listening to directions and takes her sweet time dying considering the method . xxunk about the guy playing xxmaj xxunk \\'s father giving us a cranky poor man \\'s xxmaj andy xxmaj griffith . xxmaj the scene where he is laughing about boy scouts dying was a weird hoot . xxmaj the xxmaj child is indeed a very bad film and is very bad even for the standards of 70 \\'s cheese if you will . xxmaj this is n\\'t a b film but more like a z film with producer xxmaj harry xxmaj novak making some money on virtually nothing . xxeos',\n",
       "  'neg')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ll.train.x_obj(i), ll.train.y_obj(i)) for i in [1,12552]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw samplers in notebook 03. For the validation set, we will simply sort the samples by length, and we begin with the longest ones for memory reasons (it's better to always have the biggest tensors first)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We sort so that all of the long documents are in the same batches.  This avoids you wasting gpu on going through padding of a shorter document mixed in with the longer documents. For validation set we can do a true sort. For training we want the order of our docs randomized so we do a \"sortish\" where we group into mega batches that are sorted and then shuffle within those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "class SortSampler(Sampler):\n",
    "    def __init__(self, data_source, key): self.data_source,self.key = data_source,key\n",
    "    def __len__(self): return len(self.data_source)\n",
    "    def __iter__(self):\n",
    "        return iter(sorted(list(range(len(self.data_source))), key=self.key, reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training set, we want some kind of randomness on top of this. So first, we shuffle the texts and build megabatches of size `50 * bs`. We sort those megabatches by length before splitting them in 50 minibatches. That way we will have randomized batches of roughly the same length.\n",
    "\n",
    "Then we make sure to have the biggest batch first and shuffle the order of the other batches. We also make sure the last batch stays at the end because its size is probably lower than batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SortishSampler(Sampler):\n",
    "    def __init__(self, data_source, key, bs):\n",
    "        self.data_source,self.key,self.bs = data_source,key,bs\n",
    "\n",
    "    def __len__(self) -> int: return len(self.data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        idxs = torch.randperm(len(self.data_source))\n",
    "        megabatches = [idxs[i:i+self.bs*50] for i in range(0, len(idxs), self.bs*50)]\n",
    "        sorted_idx = torch.cat([tensor(sorted(s, key=self.key, reverse=True)) for s in megabatches])\n",
    "        batches = [sorted_idx[i:i+self.bs] for i in range(0, len(sorted_idx), self.bs)]\n",
    "        max_idx = torch.argmax(tensor([self.key(ck[0]) for ck in batches]))  # find the chunk with the largest key,\n",
    "        batches[0],batches[max_idx] = batches[max_idx],batches[0]            # then make sure it goes first.\n",
    "        batch_idxs = torch.randperm(len(batches)-2)\n",
    "        sorted_idx = torch.cat([batches[i+1] for i in batch_idxs]) if len(batches) > 1 else LongTensor([])\n",
    "        sorted_idx = torch.cat([batches[0], sorted_idx, batches[-1]])\n",
    "        return iter(sorted_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding: we had the padding token (that as an id of 1) at the end of each sequence to make them all the same size when batching them. Note that we need padding at the end to be able to use `PyTorch` convenience functions that will let us ignore that padding (see 12c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pad_collate(samples, pad_idx=1, pad_first=False):\n",
    "    max_len = max([len(s[0]) for s in samples])\n",
    "    res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: res[i, -len(s[0]):] = LongTensor(s[0])\n",
    "        else:         res[i, :len(s[0]) ] = LongTensor(s[0])\n",
    "    return res, tensor([s[1] for s in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "train_sampler = SortishSampler(ll.train.x, key=lambda t: len(ll.train[int(t)][0]), bs=bs)\n",
    "train_dl = DataLoader(ll.train, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(train_dl)\n",
    "x,y = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3311, 1782, 1577, 1399, 1371], 1016)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = []\n",
    "for i in range(x.size(0)): lengths.append(x.size(1) - (x[i]==1).sum().item())\n",
    "lengths[:5], lengths[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last one is the minimal length. This is the first batch so it has the longest sequence, but if look at the next one that is more random, we see lengths are roughly the sames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([128, 128, 128, 127, 127], 115)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter_dl)\n",
    "lengths = []\n",
    "for i in range(x.size(0)): lengths.append(x.size(1) - (x[i]==1).sum().item())\n",
    "lengths[:5], lengths[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the padding at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    7, 4894,  ...,  754,    9,    3],\n",
       "        [   2,   42,   13,  ...,  183,    9,    3],\n",
       "        [   2,    7,   19,  ...,   15,    9,    3],\n",
       "        ...,\n",
       "        [   2,    7,  117,  ...,    1,    1,    1],\n",
       "        [   2,    7,    8,  ...,    1,    1,    1],\n",
       "        [   2,    7,   16,  ...,    1,    1,    1]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we add a convenience function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_clas_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    train_sampler = SortishSampler(train_ds.x, key=lambda t: len(train_ds.x[t]), bs=bs)\n",
    "    valid_sampler = SortSampler(valid_ds.x, key=lambda t: len(valid_ds.x[t]))\n",
    "    return (DataLoader(train_ds, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, sampler=valid_sampler, collate_fn=pad_collate, **kwargs))\n",
    "\n",
    "def clas_databunchify(sd, bs, **kwargs):\n",
    "    return DataBunch(*get_clas_dls(sd.train, sd.valid, bs, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt = 64,70\n",
    "data = clas_databunchify(ll, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python notebook2script.py 12_text.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
